{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#nlp=spacy.load('en_core_web_sm')\n",
    "nlp=spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.8963e-01 -4.0309e-01  3.5350e-01 -4.7907e-01 -4.3311e-01  2.3857e-01\n",
      "  2.6962e-01  6.4332e-02  3.0767e-01  1.3712e+00 -3.7582e-01 -2.2713e-01\n",
      " -3.5657e-01 -2.5355e-01  1.7543e-02  3.3962e-01  7.4723e-02  5.1226e-01\n",
      " -3.9759e-01  5.1333e-03 -3.0929e-01  4.8911e-02 -1.8610e-01 -4.1702e-01\n",
      " -8.1639e-01 -1.6908e-01 -2.6246e-01 -1.5983e-02  1.2479e-01 -3.7276e-02\n",
      " -5.7125e-01 -1.6296e-01  1.2376e-01 -5.5464e-02  1.3244e-01  2.7519e-02\n",
      "  1.2592e-01 -3.2722e-01 -4.9165e-01 -3.5559e-01 -3.0630e-01  6.1185e-02\n",
      " -1.6932e-01 -6.2405e-02  6.5763e-01 -2.7925e-01 -3.0450e-03 -2.2400e-02\n",
      " -2.8015e-01 -2.1975e-01 -4.3188e-01  3.9864e-02 -2.2102e-01 -4.2693e-02\n",
      "  5.2748e-02  2.8726e-01  1.2315e-01 -2.8662e-02  7.8294e-02  4.6754e-01\n",
      " -2.4589e-01 -1.1064e-01  7.2250e-02 -9.4980e-02 -2.7548e-01 -5.4097e-01\n",
      "  1.2823e-01 -8.2408e-02  3.1035e-01 -6.3394e-02 -7.3755e-01 -5.4992e-01\n",
      "  9.9999e-02 -2.0758e-01 -3.9674e-02  2.0664e-01 -9.7557e-02 -3.7092e-01\n",
      "  2.7901e-01 -6.2218e-01 -1.0280e-01  2.3271e-01  4.3838e-01  3.2445e-02\n",
      " -2.9866e-01 -7.3611e-02  7.1594e-01  1.4241e-01  2.7770e-01 -3.9892e-01\n",
      "  3.6656e-02  1.5759e-01  8.2014e-02 -5.7343e-01  3.5457e-01  2.2491e-01\n",
      " -6.2699e-01 -8.8106e-02  2.4361e-01  3.8533e-01 -1.4083e-01  1.7691e-01\n",
      "  7.0897e-02  1.7951e-01 -4.5907e-01 -8.2120e-01 -2.6631e-02  6.2549e-02\n",
      "  4.2415e-01 -8.9630e-02 -2.4654e-01  1.4156e-01  4.0187e-01 -4.1232e-01\n",
      "  8.4516e-02 -1.0626e-01  7.3145e-01  1.9217e-01  1.4240e-01  2.8511e-01\n",
      " -2.9454e-01 -2.1948e-01  9.0460e-01 -1.9098e-01 -1.0340e+00 -1.5754e-01\n",
      " -1.1964e-01  4.9888e-01 -1.0624e+00 -3.2820e-01 -1.1232e-02 -7.9482e-01\n",
      "  3.7275e-01 -6.8710e-03 -2.5772e-01 -4.7005e-01 -4.1387e-01 -6.4089e-02\n",
      " -2.8033e-01 -4.0778e-02 -2.4866e+00  6.2494e-03 -1.0210e-02  1.2752e-01\n",
      "  3.4965e-01 -1.2571e-01  3.1570e-01  4.1926e-01  2.0056e-01 -5.5984e-01\n",
      " -2.2801e-01  1.2012e-01 -2.0518e-03 -8.9764e-02 -8.0373e-02  1.1969e-02\n",
      " -2.6978e-01  3.4829e-01  7.3664e-03 -1.1137e-01  6.3410e-01  3.8449e-01\n",
      " -6.2248e-01  4.1145e-02  2.5922e-01  6.5811e-01 -4.9548e-01 -1.3030e-01\n",
      " -3.8279e-01  1.1156e-01 -4.3085e-01  3.4473e-01  2.7109e-02 -2.5108e-01\n",
      " -2.8011e-01  2.1662e-01  3.2660e-01  5.5895e-02  7.6077e-02 -5.2480e-02\n",
      "  4.5928e-02 -2.5266e-01  5.2845e-01 -1.3145e-01 -1.2453e-01  4.0556e-01\n",
      "  3.1877e-01  2.4415e-02 -2.2620e-01 -6.1960e-01 -4.0886e-01 -3.5534e-02\n",
      " -5.5123e-03  2.3438e-01  8.7854e-01 -2.5161e-01  4.0600e-01 -4.4284e-01\n",
      "  3.4934e-01 -5.6429e-01 -2.3676e-01  6.2199e-01 -2.8175e-01  4.2024e-01\n",
      "  1.0043e-01 -1.4720e-01  4.9593e-01 -3.5850e-01 -1.3998e-01 -2.7494e-01\n",
      "  2.3827e-01  5.7268e-01  7.9025e-02  1.7872e-02 -2.1829e-01  5.5050e-02\n",
      " -5.4200e-01  1.6788e-01  3.9065e-01  3.0209e-01  2.3040e-01 -3.9351e-02\n",
      " -2.1078e-01 -2.7224e-01  1.6907e-01  5.4819e-01  9.4888e-02  7.9798e-01\n",
      " -6.6158e-02  1.9844e-01  2.0307e-01  4.4808e-02 -1.0240e-01 -6.9909e-02\n",
      " -3.6756e-02  9.5159e-02 -2.7830e-01 -1.0597e-01 -1.6276e-01 -1.8211e-01\n",
      " -3.1897e-01 -2.1633e-01  1.4994e-01 -7.2057e-02  2.2264e-01 -4.5551e-01\n",
      "  3.0341e-01  1.8431e-01  2.1681e-01 -3.1940e-01  2.6426e-01  5.8106e-01\n",
      "  5.4635e-02  6.3238e-01  4.3169e-01  9.0343e-02  1.9494e-01  3.5483e-01\n",
      " -2.0706e-02 -7.3117e-01  1.2941e-01  1.7418e-01 -1.5065e-01  5.3355e-02\n",
      "  4.4794e-02 -1.6600e-01  2.2007e-01 -5.3970e-01 -2.4968e-01 -2.6464e-01\n",
      " -5.5515e-01  5.8242e-01  2.2295e-01  2.4433e-01  4.5275e-01  3.4693e-01\n",
      "  1.2255e-01 -3.9059e-02 -3.2749e-01 -2.7891e-01  1.3766e-01  3.8392e-01\n",
      "  1.0543e-03 -1.0242e-02  4.9205e-01 -1.7922e-01  4.1215e-02  1.3547e-01\n",
      " -2.0598e-01 -2.3194e-01 -7.7701e-01 -3.8237e-01 -7.6383e-01  1.9418e-01\n",
      " -1.5441e-01  8.9740e-01  3.0626e-01  4.0376e-01  2.1738e-01 -3.8050e-01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#matrics to word in WE\n",
    "print(nlp('lion').vector)\n",
    "len(nlp('lion').vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lion lion 1.0\n",
      "lion cat 0.52654374\n",
      "lion king 0.4747401\n",
      "lion mat 0.22247517\n",
      "lion queen 0.3698343\n",
      "===========================\n",
      "cat lion 0.52654374\n",
      "cat cat 1.0\n",
      "cat king 0.27846104\n",
      "cat mat 0.3010159\n",
      "cat queen 0.31004056\n",
      "===========================\n",
      "king lion 0.4747401\n",
      "king cat 0.27846104\n",
      "king king 1.0\n",
      "king mat 0.22552009\n",
      "king queen 0.72526103\n",
      "===========================\n",
      "mat lion 0.22247517\n",
      "mat cat 0.3010159\n",
      "mat king 0.22552009\n",
      "mat mat 1.0\n",
      "mat queen 0.20748185\n",
      "===========================\n",
      "queen lion 0.3698343\n",
      "queen cat 0.31004056\n",
      "queen king 0.72526103\n",
      "queen mat 0.20748185\n",
      "queen queen 1.0\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "#similar between some words\n",
    "token=nlp('lion cat king mat queen ')\n",
    "for token1 in token:\n",
    "    for token2 in token:\n",
    "        print(token1.text,token2.text,token1.similarity(token2))\n",
    "    print('===========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7252610345406867"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(u'king').similarity(nlp(u'queen')) #similar between two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp('hoor').has_vector #Is it in the embedding word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8388529200087637"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp('i love ali').similarity(nlp('i heat ali')) # Inaccurate with sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.03302498180054884"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp('pace').similarity(nlp('pice'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(684830, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of words to train in spacy \n",
    "nlp.vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = True False 6.4231944\n",
      "heat = True False 7.1336703\n",
      "aya = True False 6.658409\n"
     ]
    }
   ],
   "source": [
    "tokens=nlp('i heat aya')\n",
    "for token in tokens:\n",
    "    print(token.text,'=',token.has_vector,token.is_oov,token.vector_norm)\n",
    "    #oov--->out of vocabulary\n",
    "    #vector_norm--->L2 for numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word queen,has similarity 0.7880843877792358\n",
      "word angry,has similarity 0.2994381785392761\n",
      "word cat,has similarity 0.27486881613731384\n",
      "word land,has similarity 0.2497202605009079\n",
      "word great,has similarity 0.22708837687969208\n",
      "word apple,has similarity 0.2124195396900177\n"
     ]
    }
   ],
   "source": [
    "#for calculations similarity between words\n",
    "from scipy import spatial\n",
    "king=nlp.vocab['king'].vector\n",
    "man=nlp.vocab['man'].vector\n",
    "woman=nlp.vocab['woman'].vector\n",
    "words=['cat','apple','land','angry','great','queen']\n",
    "\n",
    "\n",
    "cosine_similarity =lambda x,y:1-spatial.distance.cosine(x,y)\n",
    "new_vector=king - man + woman\n",
    "compute_similarity=[]\n",
    "\n",
    "for word in words:\n",
    "    similarity=cosine_similarity(new_vector,nlp.vocab[word].vector)\n",
    "    compute_similarity.append((word,similarity))\n",
    "\n",
    "compute_similarity=sorted(compute_similarity,key=lambda items : -items[1])\n",
    "\n",
    "for a,b in compute_similarity:\n",
    "    print(f'word {a},has similarity {b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7252610345406867, 0.36440459541217857, 0.36094376072316603, 0.30589936639960613, 0.278461030411203, 0.2349444378309155]\n"
     ]
    }
   ],
   "source": [
    "words=['cat','apple','land','angry','great','queen']\n",
    "token=nlp('king')\n",
    "similarity=[]\n",
    "for word in words:\n",
    "    similarity.append(token.similarity(nlp(word)))\n",
    "print(sorted(similarity,reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
