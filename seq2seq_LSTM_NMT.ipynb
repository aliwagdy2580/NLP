{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_nmt.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPd/nNHl+ZE8/xTG8fRpbdL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliwagdy2580/NLP/blob/main/seq2seq_LSTM_NMT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsLSt1W9pDG-"
      },
      "source": [
        "# Seq2seq models tutorial\n",
        "\n",
        "In this tutorial we go through the details of building seq2seq models in Keras, and explore different models for different problem setups.\n",
        "\n",
        "This tutorial includes:\n",
        "\n",
        "- Vanilla seq2seq models for NMT.\n",
        "- Effect of Embedding layer with zero masking.\n",
        "- Adding attention + visualization: Luong vs. Bahdanau attention with LSTM.\n",
        "- Char level models for OOV applications (spelling correction, chatbots).\n",
        "- Issues with char models and possible fixes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHdvJ75jpKE4"
      },
      "source": [
        "# Vanilla seq2seq\n",
        "\n",
        "We will build a Neural Machine Translation (NMT) model based on the following paper: [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
        "\n",
        "![Vanilla seq2seq](https://d3i71xaburhd42.cloudfront.net/673fa8ca55db79acdd88d50b465ec4580166fe09/2-Figure1-1.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9k2YmYoKnnBh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "888bc1dc-1198-447a-f3e4-d63a053eec7a"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from string import digits\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from keras.layers import Input, LSTM, Embedding, Dense, Bidirectional, Concatenate, Dot, Activation, TimeDistributed\n",
        "from keras.models import Model\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgYRKLVHriWW"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADuphMW9q6S5",
        "outputId": "88356809-d82c-4e82-87ce-9101187aaf65"
      },
      "source": [
        "!wget http://www.manythings.org/anki/fra-eng.zip\n",
        "!unzip fra-eng.zip -d fra-eng\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-03 19:35:25--  http://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.21.92.44, 172.67.186.54, 2606:4700:3030::6815:5c2c, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.21.92.44|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6451478 (6.2M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "\rfra-eng.zip           0%[                    ]       0  --.-KB/s               \rfra-eng.zip         100%[===================>]   6.15M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-09-03 19:35:25 (111 MB/s) - ‘fra-eng.zip’ saved [6451478/6451478]\n",
            "\n",
            "Archive:  fra-eng.zip\n",
            "  inflating: fra-eng/_about.txt      \n",
            "  inflating: fra-eng/fra.txt         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyMmolmprkKR"
      },
      "source": [
        "data_path = 'fra-eng/fra.txt'\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "inputs = []\n",
        "targets = []\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "  input, target, _ = line.split('\\t')  \n",
        "  inputs.append(input)\n",
        "  targets.append(target)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0lv_lKbslYy"
      },
      "source": [
        "lines=pd.DataFrame({'input':inputs,'target':targets})"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xf9UEEjrtiO4"
      },
      "source": [
        "num_samples = 10000\n",
        "lines = lines[0:num_samples]\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js_FE4xtG_KA",
        "outputId": "fa9c6e21-f60a-4d83-91bf-5c60cd8fe042"
      },
      "source": [
        "lines.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "oicoj3SyG_HF",
        "outputId": "995b6a8f-4616-4697-df66-3a38f65c3fdf"
      },
      "source": [
        "lines.head(10)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Va !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Marche.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Bouge !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Cours !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Courez !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Prenez vos jambes à vos cous !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Run!</td>\n",
              "      <td>File !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Filez !</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  input                          target\n",
              "0   Go.                            Va !\n",
              "1   Go.                         Marche.\n",
              "2   Go.                         Bouge !\n",
              "3   Hi.                         Salut !\n",
              "4   Hi.                          Salut.\n",
              "5  Run!                         Cours !\n",
              "6  Run!                        Courez !\n",
              "7  Run!  Prenez vos jambes à vos cous !\n",
              "8  Run!                          File !\n",
              "9  Run!                         Filez !"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qH0UyxZKezV"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6mjJU6-17dt"
      },
      "source": [
        "def cleanup(lines):\n",
        "  # Since we work on word level, if we normalize the text to lower case, this will reduce the vocabulary. It's easy to recover the case later. \n",
        "  lines.input=lines.input.apply(lambda x: x.lower())\n",
        "  lines.target=lines.target.apply(lambda x: x.lower())\n",
        "\n",
        "  # To help the model capture the word separations, mark the comma with special token:\n",
        "  lines.input=lines.input.apply(lambda x:re.sub(\"'\",'',x)).apply(lambda x: re.sub(\",\", ' COMMA', x))\n",
        "  lines.target=lines.target.apply(lambda x:re.sub(\"'\",'',x)).apply(lambda x: re.sub(\",\", ' COMMA', x))\n",
        "\n",
        "  # Clean up punctuations and digits. Such special chars are common to both domains, and can just be copied with no error.\n",
        "  exclude = set(string.punctuation)\n",
        "  lines.input=lines.input.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
        "  lines.target=lines.target.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
        "\n",
        "  remove_digits = str.maketrans('', '', digits)\n",
        "  lines.input=lines.input.apply(lambda x: x.translate(remove_digits))\n",
        "  lines.target=lines.target.apply(lambda x: x.translate(remove_digits))\n",
        "  #return lines\n",
        "    "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3NWR2S0FnGl"
      },
      "source": [
        "st_tok='START_'\n",
        "end_tok='_END'\n",
        "def data_prep(lines):\n",
        "  cleanup(lines)\n",
        "  lines.target=lines.target.apply(lambda x: st_tok + ' ' + x + ' ' + end_tok)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKJpTFC9KjIt"
      },
      "source": [
        "To help the model identiy the start and end of sequence, we mark them by special tokens. This will help the decoding process later:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBUYKRpqFnJc"
      },
      "source": [
        "data_prep(lines)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "zjllZhyqKE1c",
        "outputId": "e93a5e37-951e-45ad-b34e-cc5c23174c4f"
      },
      "source": [
        "lines.head(10)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>go</td>\n",
              "      <td>START_ va  _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>go</td>\n",
              "      <td>START_ marche _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>go</td>\n",
              "      <td>START_ bouge  _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hi</td>\n",
              "      <td>START_ salut  _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hi</td>\n",
              "      <td>START_ salut _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>run</td>\n",
              "      <td>START_ cours  _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>run</td>\n",
              "      <td>START_ courez  _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>run</td>\n",
              "      <td>START_ prenez vos jambes à vos cous  _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>run</td>\n",
              "      <td>START_ file  _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>run</td>\n",
              "      <td>START_ filez  _END</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  input                                     target\n",
              "0    go                            START_ va  _END\n",
              "1    go                         START_ marche _END\n",
              "2    go                         START_ bouge  _END\n",
              "3    hi                         START_ salut  _END\n",
              "4    hi                          START_ salut _END\n",
              "5   run                         START_ cours  _END\n",
              "6   run                        START_ courez  _END\n",
              "7   run  START_ prenez vos jambes à vos cous  _END\n",
              "8   run                          START_ file  _END\n",
              "9   run                         START_ filez  _END"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_Ple2P5KmVN"
      },
      "source": [
        "# Word level model (word2word)\n",
        "\n",
        "In this section we process the text at the word level, both for `input` and `target` domains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh5nJrDqFnML"
      },
      "source": [
        "def tok_split_word2word(data):\n",
        "  return data.split()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ealuqtopFtrc"
      },
      "source": [
        "tok_split_fn = tok_split_word2word"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgUaBMaMFtwi"
      },
      "source": [
        "def data_stats(lines, input_tok_split_fn, target_tok_split_fn):\n",
        "  input_tokens=set()\n",
        "  for line in lines.input:\n",
        "      for tok in input_tok_split_fn(line):\n",
        "          if tok not in input_tokens:\n",
        "              input_tokens.add(tok)\n",
        "      \n",
        "  target_tokens=set()\n",
        "  for line in lines.target:\n",
        "      for tok in target_tok_split_fn(line):\n",
        "          if tok not in target_tokens:\n",
        "              target_tokens.add(tok)\n",
        "  input_tokens = sorted(list(input_tokens))\n",
        "  target_tokens = sorted(list(target_tokens))\n",
        "\n",
        "  num_encoder_tokens = len(input_tokens)\n",
        "  num_decoder_tokens = len(target_tokens)\n",
        "  max_encoder_seq_length = np.max([len(input_tok_split_fn(l)) for l in lines.input])\n",
        "  max_decoder_seq_length = np.max([len(target_tok_split_fn(l)) for l in lines.target])\n",
        "\n",
        "  return input_tokens, target_tokens, num_encoder_tokens, num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l28ARUZ_SVaR",
        "outputId": "ea12280c-c232-4827-8532-bf380ff75132"
      },
      "source": [
        "input_tokens,target_tokens,num_encoder_tokens,num_decoder_tokens,max_encoder_seq_length,max_decoder_seq_length=data_stats(lines,input_tok_split_fn=tok_split_fn,target_tok_split_fn=tok_split_fn)\n",
        "print('Number of samples:', len(lines))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 10000\n",
            "Number of unique input tokens: 2017\n",
            "Number of unique output tokens: 4472\n",
            "Max sequence length for inputs: 5\n",
            "Max sequence length for outputs: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCDYd0E0Vgzm"
      },
      "source": [
        "#Vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAn2yXnlVj3h"
      },
      "source": [
        "When building our vocab, we should keep a special entry for the PAD \n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "token, _always_ at the entry 0 of the vocab table. Of course this only happens when we pad with 0's. In general the padded token should be reserved, and NEVER used by any other token (otherwise we won't tell the difference between REAL 0 and PAD 0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lt_7KwGPFtyg"
      },
      "source": [
        "pad_tok = 'PAD'\n",
        "sep_tok = ' '\n",
        "special_tokens = [pad_tok, sep_tok, st_tok, end_tok] \n",
        "num_encoder_tokens += len(special_tokens)\n",
        "num_decoder_tokens += len(special_tokens)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPbomiK8FyZk"
      },
      "source": [
        "\n",
        "def vocab(input_tokens, target_tokens):\n",
        "  \n",
        "  input_token_index = {}\n",
        "  target_token_index = {}\n",
        "  for i,tok in enumerate(special_tokens):\n",
        "    input_token_index[tok] = i\n",
        "    target_token_index[tok] = i \n",
        "\n",
        "  offset = len(special_tokens)\n",
        "  for i, tok in enumerate(input_tokens):\n",
        "    input_token_index[tok] = i+offset\n",
        "\n",
        "  for i, tok in enumerate(target_tokens):\n",
        "    target_token_index[tok] = i+offset\n",
        "   \n",
        "  # Reverse-lookup token index to decode sequences back to something readable.\n",
        "  reverse_input_tok_index = dict(\n",
        "      (i, tok) for tok, i in input_token_index.items())\n",
        "  reverse_target_tok_index = dict(\n",
        "      (i, tok) for tok, i in target_token_index.items())\n",
        "  return input_token_index, target_token_index, reverse_input_tok_index, reverse_target_tok_index"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMug8-dTFycU"
      },
      "source": [
        "input_token_index, target_token_index, reverse_input_tok_index, reverse_target_tok_index = vocab(input_tokens, target_tokens)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC1lu-ppY1JI"
      },
      "source": [
        "# Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJbnztywY1L3"
      },
      "source": [
        "Also, when we vectorize, if we pad 0 (or any token), we need to set the corresponding target.\n",
        "\n",
        "Below, we should set all `decoder_target_data` to the OHE corresponding the entry 0 (or whatever entry of PAD).\n",
        "\n",
        "For example, if we pad with 0, and 0 is the first entry in vocab, then it's `decoder_target_data` must be [1,0,0,....0].\n",
        "However, this is not needed if we use `mask_zero` in `Embedding`, or using a masking layer with LSTM, because those values wont be fed fwd anyway, and will not be considered in loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjpfqwRlF3ab"
      },
      "source": [
        "max_encoder_seq_length = 16\n",
        "max_decoder_seq_length = 16"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8npCTEaGF3c9"
      },
      "source": [
        "def init_model_inputs(lines, max_encoder_seq_length, max_decoder_seq_length, num_decoder_tokens):\n",
        "  encoder_input_data = np.zeros(\n",
        "      (len(lines.input), max_encoder_seq_length),\n",
        "      dtype='float32')\n",
        "  decoder_input_data = np.zeros(\n",
        "      (len(lines.target), max_decoder_seq_length),\n",
        "      dtype='float32')\n",
        "  decoder_target_data = np.zeros(\n",
        "      (len(lines.target), max_decoder_seq_length, num_decoder_tokens),\n",
        "      dtype='float32')\n",
        "  \n",
        "  return encoder_input_data, decoder_input_data, decoder_target_data"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyfeW6UAK1ZQ"
      },
      "source": [
        "  encoder_input_data, decoder_input_data, decoder_target_data = init_model_inputs(lines, max_encoder_seq_length, max_decoder_seq_length, num_decoder_tokens)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zZLDOeifMyv"
      },
      "source": [
        "def vectorize(lines, max_encoder_seq_length, max_decoder_seq_length, num_decoder_tokens, input_tok_split_fn, target_tok_split_fn):\n",
        "  encoder_input_data, decoder_input_data, decoder_target_data = init_model_inputs(lines, max_encoder_seq_length, max_decoder_seq_length, num_decoder_tokens)\n",
        "  for i, (input_text, target_text) in enumerate(zip(lines.input, lines.target)):\n",
        "      for t, tok in enumerate(input_tok_split_fn(input_text)):\n",
        "          encoder_input_data[i, t] = input_token_index[tok]\n",
        "      encoder_input_data[i, t+1:] = input_token_index[pad_tok]\n",
        "      for t, tok in enumerate(target_tok_split_fn(target_text)):\n",
        "          # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "          decoder_input_data[i, t] = target_token_index[tok]         \n",
        "          if t > 0:\n",
        "              # decoder_target_data will be ahead by one timestep\n",
        "              # and will not include the start character.\n",
        "              decoder_target_data[i, t - 1, target_token_index[tok]] = 1.\n",
        "      decoder_input_data[i, t+1:] = target_token_index[pad_tok] \n",
        "      decoder_target_data[i, t:, target_token_index[pad_tok]] = 1.          \n",
        "              \n",
        "  return encoder_input_data, decoder_input_data, decoder_target_data              "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhSOAQU5F3mM"
      },
      "source": [
        "encoder_input_data, decoder_input_data, decoder_target_data  = vectorize(lines, max_encoder_seq_length, max_decoder_seq_length, num_decoder_tokens, input_tok_split_fn=tok_split_fn, target_tok_split_fn=tok_split_fn)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4z4mLCTlfec"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcSw55d3Fyer"
      },
      "source": [
        "emb_sz = 50"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5H-mz8DlrYk"
      },
      "source": [
        "#### Encoder model\n",
        "\n",
        "The LSTM is characterized by two internal tensors:\n",
        "- cell state $C_t$: what to copy from past $C_{t-1}$ (after forgetting some info through sigmoid $f_t$) and what to take from the merge of last $h_{t-1}$ and current input $x_t$ \n",
        "- hidden state $h_t$: same as hidden state of normal RNNs. This affects the output of the LSTM Cell\n",
        "\n",
        "We usually refer to the combination $[C_t, h_t]$ as the LSTM state at time step $t$.\n",
        "\n",
        "So the inputs to LSTM are:\n",
        "- $[C_{t-1}, h_{t-1}]$ = previous state\n",
        "- $x_t$ = input tokens\n",
        "\n",
        "And the outputs are:\n",
        "- $[C_{t}, h_{t}]$\n",
        "\n",
        "\n",
        "![LSTM Cell](https://www.researchgate.net/profile/Savvas_Varsamopoulos/publication/329362532/figure/fig5/AS:699592479870977@1543807253596/Structure-of-the-LSTM-cell-and-equations-that-describe-the-gates-of-an-LSTM-cell.jpg)\n",
        "\n",
        "According to this nice [illustration](https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/), when we feedfwd into LSTM, we can control the return from keras as follows:\n",
        "\n",
        "- `return_state`: asks LSTM to return both states: $C_t$ and $h_t$. Three outputs are returned: $h_t$, $h_t$, $C_t$\n",
        "- `return_sequence`: asks the unrollled LSTM to return its state $[C_t, h_t]$ for all $t \\in [t_0,T]$. In this case 3 outputs are returned: [$h_{t_0}$, .., $h_T$], $h_T$, $C_T$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3shCNcglygg"
      },
      "source": [
        "In the encoder model, we are interested in the last combined state [$h_t$, $C_t$]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d05a1Mx8l2dV"
      },
      "source": [
        "#### Decoder model\n",
        "\n",
        "In the decoder, we are interested in:\n",
        "- intialize its hidden state from where the encoder ended `initial_state`=[$h_t$, $C_t$]\n",
        "- the sequence outputs [$h_{t_0}$, .., $h_T$] returned thanks to `return_states`, which will be used to generate the output tokens through the final `softmax`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX4H7Q5W7HI6"
      },
      "source": [
        "The decoder is just another LSTM with the normal 2 inputs:\n",
        "- $[C_{t-1}, h_{t-1}]$, where $[C_{0}, h_{0}]$ = encoder final state $[C_{T}, h_{T}]$\n",
        "- $x_t$ = target tokens, shifted by one `start_tok` = `START_`, where $x_0$ = `START_` to kick-start the decoder input, coupled with the encoder final state.\n",
        "\n",
        "Feeding the GT target tokens during training is called ___Teacher forcing___. This won't be straight forward using Keras, since it requires a dynamic graph to handle the _if_ condition of teacher forcing or free run mode (like in inference mode, taking the previous output). May be later using Eager execution.\n",
        "\n",
        "During inference $x_t$ of the decoder, will just be the previous output $x_{t-1}$.\n",
        "\n",
        "![enc-dec](https://miro.medium.com/max/3170/1*sO-SP58T4brE9EHazHSeGA.png)\n",
        "\n",
        "Sometimes, it's helpful to switch between the GT target_tokens are inputs to the decoder during training, and sometimes take the previous output $x_{t-1}$ as will happen in the inference, so that the decoder is trained to recover from his mistakes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vds3U8Oxmkr3"
      },
      "source": [
        "def seq2seq(num_decoder_tokens, num_encoder_tokens, emb_sz, lstm_sz):\n",
        "  #encoder\n",
        "  encoder_inputs = Input(shape=(None,)) #create new layer\n",
        "  en_x=  Embedding(num_encoder_tokens, emb_sz)(encoder_inputs) #embedding_vector  [vocab_sz, num_latent_factore]  create new layer\n",
        "  encoder = LSTM(lstm_sz, return_state=True)  #create new layer\n",
        "  encoder_outputs, state_h, state_c = encoder(en_x)\n",
        "  # We discard `encoder_outputs` and only keep the states.\n",
        "  encoder_states = [state_h, state_c]  #last state\n",
        "  \n",
        "  # Encoder model\n",
        "  encoder_model = Model(encoder_inputs, encoder_states)\n",
        "  \n",
        "  #decoder5\n",
        "  # Set up the decoder, using `encoder_states` as initial state.\n",
        "  decoder_inputs = Input(shape=(None,)) #create new layer\n",
        "\n",
        "  dex=  Embedding(num_decoder_tokens, emb_sz) # new embedding_vector  [vocab_sz, num_latent_factore]  create new layer\n",
        "\n",
        "  final_dex= dex(decoder_inputs)\n",
        "\n",
        "\n",
        "  decoder_lstm = LSTM(lstm_sz, return_sequences=True, return_state=True) #create new layer\n",
        "\n",
        "  decoder_outputs, _, _ = decoder_lstm(final_dex,\n",
        "                                      initial_state=encoder_states)\n",
        "\n",
        "  decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "  model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "  model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "\n",
        "  \n",
        "  # Decoder model: Re-build based on explicit state inputs. Needed for step-by-step inference:\n",
        "  decoder_state_input_h = Input(shape=(lstm_sz,))\n",
        "  decoder_state_input_c = Input(shape=(lstm_sz,))\n",
        "  decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]# last state\n",
        "\n",
        "  decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex, initial_state=decoder_states_inputs)\n",
        "  decoder_states2 = [state_h2, state_c2]\n",
        "  decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "  decoder_model = Model(\n",
        "  [decoder_inputs] + decoder_states_inputs,\n",
        "  [decoder_outputs2] + decoder_states2)  \n",
        "\n",
        "  return model, encoder_model, decoder_model\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qrb6x2e9Qm1",
        "outputId": "252e3c05-dc47-4990-c535-0c2c8b7bb9de"
      },
      "source": [
        "model, encoder_model, decoder_model = seq2seq(num_decoder_tokens, num_encoder_tokens, emb_sz, emb_sz)\n",
        "print(model.summary())\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 50)     101050      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 50)     223800      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 50), (None,  20200       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 50), ( 20200       embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 4476)   228276      lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 593,526\n",
            "Trainable params: 593,526\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5ZY5IwEzH1f"
      },
      "source": [
        "#Fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXQ5L5Bv9Giv",
        "outputId": "3d53b856-9850-4c09-fe6b-4eb5e55fbaa7"
      },
      "source": [
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=64,\n",
        "          epochs=20,\n",
        "          validation_split=0.05)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "149/149 [==============================] - 15s 43ms/step - loss: 3.0031 - acc: 0.7414 - val_loss: 1.7306 - val_acc: 0.7230\n",
            "Epoch 2/20\n",
            "149/149 [==============================] - 5s 36ms/step - loss: 1.3302 - acc: 0.8027 - val_loss: 1.5584 - val_acc: 0.7796\n",
            "Epoch 3/20\n",
            "149/149 [==============================] - 5s 36ms/step - loss: 1.1860 - acc: 0.8245 - val_loss: 1.4828 - val_acc: 0.7812\n",
            "Epoch 4/20\n",
            "149/149 [==============================] - 5s 36ms/step - loss: 1.1229 - acc: 0.8286 - val_loss: 1.4643 - val_acc: 0.7820\n",
            "Epoch 5/20\n",
            "149/149 [==============================] - 5s 36ms/step - loss: 1.0836 - acc: 0.8308 - val_loss: 1.4603 - val_acc: 0.7850\n",
            "Epoch 6/20\n",
            "149/149 [==============================] - 5s 36ms/step - loss: 1.0532 - acc: 0.8342 - val_loss: 1.4399 - val_acc: 0.7921\n",
            "Epoch 7/20\n",
            "149/149 [==============================] - 5s 36ms/step - loss: 1.0277 - acc: 0.8380 - val_loss: 1.4312 - val_acc: 0.7934\n",
            "Epoch 8/20\n",
            "149/149 [==============================] - 5s 36ms/step - loss: 1.0055 - acc: 0.8419 - val_loss: 1.4228 - val_acc: 0.7926\n",
            "Epoch 9/20\n",
            "149/149 [==============================] - 5s 36ms/step - loss: 0.9858 - acc: 0.8446 - val_loss: 1.4086 - val_acc: 0.7959\n",
            "Epoch 10/20\n",
            "149/149 [==============================] - 5s 36ms/step - loss: 0.9678 - acc: 0.8471 - val_loss: 1.4046 - val_acc: 0.8099\n",
            "Epoch 11/20\n",
            "149/149 [==============================] - 5s 36ms/step - loss: 0.9512 - acc: 0.8492 - val_loss: 1.3996 - val_acc: 0.8050\n",
            "Epoch 12/20\n",
            "149/149 [==============================] - 5s 36ms/step - loss: 0.9356 - acc: 0.8512 - val_loss: 1.3976 - val_acc: 0.8117\n",
            "Epoch 13/20\n",
            "149/149 [==============================] - 5s 37ms/step - loss: 0.9202 - acc: 0.8531 - val_loss: 1.3777 - val_acc: 0.8064\n",
            "Epoch 14/20\n",
            "149/149 [==============================] - 5s 36ms/step - loss: 0.9059 - acc: 0.8548 - val_loss: 1.3556 - val_acc: 0.8140\n",
            "Epoch 15/20\n",
            "149/149 [==============================] - 5s 36ms/step - loss: 0.8926 - acc: 0.8569 - val_loss: 1.3477 - val_acc: 0.8139\n",
            "Epoch 16/20\n",
            "149/149 [==============================] - 5s 36ms/step - loss: 0.8795 - acc: 0.8582 - val_loss: 1.3411 - val_acc: 0.8140\n",
            "Epoch 17/20\n",
            "149/149 [==============================] - 5s 36ms/step - loss: 0.8664 - acc: 0.8595 - val_loss: 1.3403 - val_acc: 0.8144\n",
            "Epoch 18/20\n",
            "149/149 [==============================] - 5s 37ms/step - loss: 0.8533 - acc: 0.8607 - val_loss: 1.3341 - val_acc: 0.8111\n",
            "Epoch 19/20\n",
            "149/149 [==============================] - 5s 36ms/step - loss: 0.8406 - acc: 0.8621 - val_loss: 1.3269 - val_acc: 0.8159\n",
            "Epoch 20/20\n",
            "149/149 [==============================] - 5s 37ms/step - loss: 0.8283 - acc: 0.8637 - val_loss: 1.3198 - val_acc: 0.8196\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdde2fab5d0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2qfTGAVzPLl"
      },
      "source": [
        "# Embedding + `mask_zero`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J37tlbPF9M4U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f8686ef-1bd1-4ae9-95d4-9b43e245cc22"
      },
      "source": [
        "def seq2seq(num_decoder_tokens, num_encoder_tokens, emb_sz, lstm_sz):\n",
        "  encoder_inputs = Input(shape=(None,))\n",
        "  en_x=  Embedding(num_encoder_tokens, emb_sz, mask_zero=True)(encoder_inputs)\n",
        "  encoder = LSTM(lstm_sz, return_state=True)\n",
        "  encoder_outputs, state_h, state_c = encoder(en_x)\n",
        "  # We discard `encoder_outputs` and only keep the states.\n",
        "  encoder_states = [state_h, state_c]\n",
        "  \n",
        "  # Encoder model\n",
        "  encoder_model = Model(encoder_inputs, encoder_states)\n",
        "  \n",
        "  \n",
        "  # Set up the decoder, using `encoder_states` as initial state.\n",
        "  decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "  dex=  Embedding(num_decoder_tokens, emb_sz, mask_zero=True)\n",
        "\n",
        "  final_dex= dex(decoder_inputs)\n",
        "\n",
        "\n",
        "  decoder_lstm = LSTM(lstm_sz, return_sequences=True, return_state=True)\n",
        "\n",
        "  decoder_outputs, _, _ = decoder_lstm(final_dex,\n",
        "                                      initial_state=encoder_states)\n",
        "\n",
        "  decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "  model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "  model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "\n",
        "  \n",
        "  # Decoder model: Re-build based on explicit state inputs. Needed for step-by-step inference:\n",
        "  decoder_state_input_h = Input(shape=(lstm_sz,))\n",
        "  decoder_state_input_c = Input(shape=(lstm_sz,))\n",
        "  decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "  decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex, initial_state=decoder_states_inputs)\n",
        "  decoder_states2 = [state_h2, state_c2]\n",
        "  decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "  decoder_model = Model(\n",
        "  [decoder_inputs] + decoder_states_inputs,\n",
        "  [decoder_outputs2] + decoder_states2)  \n",
        "\n",
        "  return model, encoder_model, decoder_model\n",
        "emb_sz = 256\n",
        "model, encoder_model, decoder_model = seq2seq(num_decoder_tokens, num_encoder_tokens, emb_sz, emb_sz)\n",
        "print(model.summary())\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=64,\n",
        "          epochs=10,\n",
        "          validation_split=0.2)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, None, 256)    517376      input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, None, 256)    1145856     input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 256), (None, 525312      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 256),  525312      embedding_3[0][0]                \n",
            "                                                                 lstm_2[0][1]                     \n",
            "                                                                 lstm_2[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 4476)   1150332     lstm_3[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 3,864,188\n",
            "Trainable params: 3,864,188\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 15s 67ms/step - loss: 1.3550 - acc: 0.3949 - val_loss: 1.3856 - val_acc: 0.3882\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 1.0427 - acc: 0.4930 - val_loss: 1.2587 - val_acc: 0.4665\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.9338 - acc: 0.5365 - val_loss: 1.1879 - val_acc: 0.5078\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.8614 - acc: 0.5608 - val_loss: 1.1432 - val_acc: 0.5272\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.8038 - acc: 0.5824 - val_loss: 1.1221 - val_acc: 0.5450\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.7545 - acc: 0.5994 - val_loss: 1.0865 - val_acc: 0.5583\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.7105 - acc: 0.6148 - val_loss: 1.0772 - val_acc: 0.5636\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.6708 - acc: 0.6290 - val_loss: 1.0630 - val_acc: 0.5762\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.6347 - acc: 0.6441 - val_loss: 1.0504 - val_acc: 0.5758\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.6005 - acc: 0.6578 - val_loss: 1.0434 - val_acc: 0.5812\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdd9cc4e890>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nazyjh1IPRPi"
      },
      "source": [
        "#### Function to generate sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5OuZpolybK_"
      },
      "source": [
        "def decode_sequence(input_seq, sep=' '):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq) # for init decoder input\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = target_token_index[st_tok] #_START\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_tok = reverse_target_tok_index[sampled_token_index]\n",
        "        decoded_sentence += sep + sampled_tok\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_tok == end_tok or\n",
        "           len(decoded_sentence) > 52):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmFiM3HAM-vp",
        "outputId": "cc8e3e70-c602-4442-dba9-c8fad92e1515",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for seq_index in range(20): #[14077,20122,40035,40064, 40056, 40068, 40090, 40095, 40100, 40119, 40131, 40136, 40150, 40153]:\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', lines.input[seq_index: seq_index + 1])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "Input sentence: 0    go\n",
            "Name: input, dtype: object\n",
            "Decoded sentence:  à la maison _END\n",
            "-\n",
            "Input sentence: 1    go\n",
            "Name: input, dtype: object\n",
            "Decoded sentence:  à la maison _END\n",
            "-\n",
            "Input sentence: 2    go\n",
            "Name: input, dtype: object\n",
            "Decoded sentence:  à la maison _END\n",
            "-\n",
            "Input sentence: 3    hi\n",
            "Name: input, dtype: object\n",
            "Decoded sentence:  à la maison _END\n",
            "-\n",
            "Input sentence: 4    hi\n",
            "Name: input, dtype: object\n",
            "Decoded sentence:  à la maison _END\n",
            "-\n",
            "Input sentence: 5    run\n",
            "Name: input, dtype: object\n",
            "Decoded sentence:  prenez la porte _END\n",
            "-\n",
            "Input sentence: 6    run\n",
            "Name: input, dtype: object\n",
            "Decoded sentence:  prenez la porte _END\n",
            "-\n",
            "Input sentence: 7    run\n",
            "Name: input, dtype: object\n",
            "Decoded sentence:  prenez la porte _END\n",
            "-\n",
            "Input sentence: 8    run\n",
            "Name: input, dtype: object\n",
            "Decoded sentence:  prenez la porte _END\n",
            "-\n",
            "Input sentence: 9    run\n",
            "Name: input, dtype: object\n",
            "Decoded sentence:  prenez la porte _END\n",
            "-\n",
            "Input sentence: 10    run\n",
            "Name: input, dtype: object\n",
            "Decoded sentence:  prenez la porte _END\n",
            "-\n",
            "Input sentence: 11    run\n",
            "Name: input, dtype: object\n",
            "Decoded sentence:  prenez la porte _END\n",
            "-\n",
            "Input sentence: 12    run\n",
            "Name: input, dtype: object\n",
            "Decoded sentence:  prenez la porte _END\n",
            "-\n",
            "Input sentence: 13    run\n",
            "Name: input, dtype: object\n",
            "Decoded sentence:  prenez la porte _END\n",
            "-\n",
            "Input sentence: 14    run\n",
            "Name: input, dtype: object\n",
            "Decoded sentence:  prenez la porte _END\n",
            "-\n",
            "Input sentence: 15    run\n",
            "Name: input, dtype: object\n",
            "Decoded sentence:  prenez la porte _END\n",
            "-\n",
            "Input sentence: 16    run\n",
            "Name: input, dtype: object\n",
            "Decoded sentence:  prenez la porte _END\n",
            "-\n",
            "Input sentence: 17    run\n",
            "Name: input, dtype: object\n",
            "Decoded sentence:  prenez la porte _END\n",
            "-\n",
            "Input sentence: 18    run\n",
            "Name: input, dtype: object\n",
            "Decoded sentence:  prenez la porte _END\n",
            "-\n",
            "Input sentence: 19    run\n",
            "Name: input, dtype: object\n",
            "Decoded sentence:  prenez la porte _END\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sglnxUpwPjuk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}